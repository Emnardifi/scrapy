{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3L2Ec_QEn-m",
        "outputId": "9d602120-0ed3-47ef-eecc-8d9f75c0a1b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting virtualenv\n",
            "  Downloading virtualenv-20.29.1-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv)\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.11/dist-packages (from virtualenv) (3.17.0)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from virtualenv) (4.3.6)\n",
            "Downloading virtualenv-20.29.1-py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: distlib, virtualenv\n",
            "Successfully installed distlib-0.3.9 virtualenv-20.29.1\n"
          ]
        }
      ],
      "source": [
        "pip install virtualenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd/content/part2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coj6WmlZE08F",
        "outputId": "a49ce3ca-3175-46e4-c363-f4cd6cccc10a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/part2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m venv venv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruiAeBHnE98m",
        "outputId": "63574a3d-ad48-42fe-a1c8-17c4d7afcb7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Command '['/content/part2/venv/bin/python3', '-m', 'ensurepip', '--upgrade', '--default-pip']' returned non-zero exit status 1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!source venv/bin/activate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0lOB-kfFAZj",
        "outputId": "1b0179c9-7d22-438d-ad96-fd57d022f2c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: venv/bin/activate: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scrapy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-1DPbJdFEpz",
        "outputId": "9c0cd247-06b0-4659-af2c-28a1209971d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scrapy\n",
            "  Downloading Scrapy-2.12.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting Twisted>=21.7.0 (from scrapy)\n",
            "  Downloading twisted-24.11.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: cryptography>=37.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (43.0.3)\n",
            "Collecting cssselect>=0.9.1 (from scrapy)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting itemloaders>=1.0.1 (from scrapy)\n",
            "  Downloading itemloaders-1.3.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting parsel>=1.5.0 (from scrapy)\n",
            "  Downloading parsel-1.10.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyOpenSSL>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2.1)\n",
            "Collecting queuelib>=1.4.2 (from scrapy)\n",
            "  Downloading queuelib-1.7.0-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting service-identity>=18.1.0 (from scrapy)\n",
            "  Downloading service_identity-24.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting w3lib>=1.17.0 (from scrapy)\n",
            "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting zope.interface>=5.1.0 (from scrapy)\n",
            "  Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protego>=0.1.15 (from scrapy)\n",
            "  Downloading Protego-0.4.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting itemadapter>=0.1.0 (from scrapy)\n",
            "  Downloading itemadapter-0.11.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2)\n",
            "Collecting tldextract (from scrapy)\n",
            "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: lxml>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (5.3.0)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from scrapy) (0.7.1)\n",
            "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
            "  Downloading PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=37.0.0->scrapy) (1.17.1)\n",
            "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (25.1.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.1)\n",
            "Collecting automat>=24.8.0 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading Automat-24.8.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting constantly>=15.1 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting hyperlink>=17.1.1 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting incremental>=24.7.0 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading incremental-24.7.2-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from Twisted>=21.7.0->scrapy) (4.12.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from zope.interface>=5.1.0->scrapy) (75.1.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (3.10)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (2.32.3)\n",
            "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (3.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=37.0.0->scrapy) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2025.1.31)\n",
            "Downloading Scrapy-2.12.0-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading itemadapter-0.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading itemloaders-1.3.2-py3-none-any.whl (12 kB)\n",
            "Downloading parsel-1.10.0-py2.py3-none-any.whl (17 kB)\n",
            "Downloading Protego-0.4.0-py2.py3-none-any.whl (8.6 kB)\n",
            "Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
            "Downloading queuelib-1.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Downloading service_identity-24.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading twisted-24.11.0-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
            "Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.8/259.8 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Automat-24.8.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
            "Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading incremental-24.7.2-py3-none-any.whl (20 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: PyDispatcher, zope.interface, w3lib, queuelib, protego, jmespath, itemadapter, incremental, hyperlink, cssselect, constantly, automat, Twisted, requests-file, parsel, tldextract, service-identity, itemloaders, scrapy\n",
            "Successfully installed PyDispatcher-2.0.7 Twisted-24.11.0 automat-24.8.1 constantly-23.10.4 cssselect-1.2.0 hyperlink-21.0.0 incremental-24.7.2 itemadapter-0.11.0 itemloaders-1.3.2 jmespath-1.0.1 parsel-1.10.0 protego-0.4.0 queuelib-1.7.0 requests-file-2.1.0 scrapy-2.12.0 service-identity-24.2.0 tldextract-5.1.3 w3lib-2.3.1 zope.interface-7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDbXAwx3FILO",
        "outputId": "326b5317-a64c-4cc3-990f-c28ea640f305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scrapy 2.12.0 - no active project\n",
            "\n",
            "Usage:\n",
            "  scrapy <command> [options] [args]\n",
            "\n",
            "Available commands:\n",
            "  bench         Run quick benchmark test\n",
            "  fetch         Fetch a URL using the Scrapy downloader\n",
            "  genspider     Generate new spider using pre-defined templates\n",
            "  runspider     Run a self-contained spider (without creating a project)\n",
            "  settings      Get settings values\n",
            "  shell         Interactive scraping console\n",
            "  startproject  Create new project\n",
            "  version       Print Scrapy version\n",
            "  view          Open URL in browser, as seen by Scrapy\n",
            "\n",
            "  [ more ]      More commands available when run from project directory\n",
            "\n",
            "Use \"scrapy <command> -h\" to see more info about a command\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy startproject bookscraper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiK8sA2UFTfP",
        "outputId": "cbffa168-4e74-44c8-b1e6-9c434f466508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Scrapy project 'bookscraper', using template directory '/usr/local/lib/python3.11/dist-packages/scrapy/templates/project', created in:\n",
            "    /content/part2/bookscraper\n",
            "\n",
            "You can start your first spider with:\n",
            "    cd bookscraper\n",
            "    scrapy genspider example example.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/part2/bookscraper/bookscraper/spiders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-ll5CEBFfib",
        "outputId": "10dec77a-c145-4152-ff4d-1c2380f3b3b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/part2/bookscraper/bookscraper/spiders\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoQCx8RHFjBh",
        "outputId": "224710e4-ad62-42e8-e81d-21211cc55368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy genspider bookspider books.toscrape.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqilGQGDF3ZT",
        "outputId": "78beec79-c018-46ac-f1ad-869f60c8c59f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created spider 'bookspider' using template 'basic' in module:\n",
            "  bookscraper.spiders.bookspider\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMBKx2OjGMKZ",
        "outputId": "df3ab95c-4bad-4362-a4c5-a12ca4e2a087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bookspider.py  __init__.py  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy shell"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcIK5ivpGYqz",
        "outputId": "3f21ec46-797a-4131-851b-d9ff89c846db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-08 10:27:36 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: bookscraper)\n",
            "2025-02-08 10:27:36 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.3, Platform Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "2025-02-08 10:27:36 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2025-02-08 10:27:36 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2025-02-08 10:27:36 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2025-02-08 10:27:36 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2025-02-08 10:27:36 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2025-02-08 10:27:36 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2025-02-08 10:27:36 [scrapy.extensions.telnet] INFO: Telnet Password: 7a66570ec62e0077\n",
            "2025-02-08 10:27:36 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage']\n",
            "2025-02-08 10:27:36 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'bookscraper',\n",
            " 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',\n",
            " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'LOGSTATS_INTERVAL': 0,\n",
            " 'NEWSPIDER_MODULE': 'bookscraper.spiders',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['bookscraper.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2025-02-08 10:27:36 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2025-02-08 10:27:36 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2025-02-08 10:27:36 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2025-02-08 10:27:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2025-02-08 10:27:36 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "\u001b[?12l\u001b[?25h[s] Available Scrapy objects:\n",
            "[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n",
            "[s]   crawler    <scrapy.crawler.Crawler object at 0x10341ea37950>\n",
            "[s]   item       {}\n",
            "[s]   settings   <scrapy.settings.Settings object at 0x10341fdad8d0>\n",
            "[s] Useful shortcuts:\n",
            "[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\n",
            "[s]   fetch(req)                  Fetch a scrapy.Request and update local objects \n",
            "[s]   shelp()           Shell help (print this help)\n",
            "[s]   view(response)    View response in a browser\n",
            "\u001b[6n\u001b[?2004h\u001b[?1l\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m1\u001b[0;38;5;28m]: \u001b[8D\u001b[8C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[8D\u001b[J\u001b[0m\u001b[?7h\u001b[?2004lWARNING: your terminal doesn't support cursor position requests (CPR).\n",
            "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m1\u001b[0;38;5;28m]: \u001b[8D\u001b[8C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[0mfetch(\u001b[0;38;5;130m'https://books.toscrape.com/'\u001b[0m)\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[44D\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m1\u001b[0;38;5;28m]: \u001b[0mfetch(\u001b[0;38;5;130m'https://books.toscrape.com/'\u001b[0m)\u001b[44D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l2025-02-08 10:30:16 [scrapy.core.engine] INFO: Spider opened\n",
            "2025-02-08 10:30:17 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://books.toscrape.com/robots.txt> (referer: None)\n",
            "2025-02-08 10:30:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://books.toscrape.com/> (referer: None)\n",
            "\n",
            "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m2\u001b[0;38;5;28m]: \u001b[8D\u001b[8C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[8D\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m2\u001b[0;38;5;28m]: \u001b[0mresponse\u001b[16D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[0m\u001b[?7h\u001b[0;38;5;88mOut[\u001b[0;91;1m2\u001b[0;38;5;88m]: \u001b[0m\u001b[0m<200 https://books.toscrape.com/>\n",
            "\n",
            "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m3\u001b[0;38;5;28m]: \u001b[8D\u001b[8C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[8D\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m3\u001b[0;38;5;28m]: \u001b[0mresponse.css(\u001b[0;38;5;130m'article.product_pod'\u001b[0m)\u001b[43D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[0m\u001b[?7h\u001b[0;38;5;88mOut[\u001b[0;91;1m3\u001b[0;38;5;88m]: \u001b[0m\u001b[0m\n",
            "[<Selector query=\"descendant-or-self::article[@class and contains(concat(' ', normalize-space(@class), ' '), ' product_pod ')]\" data='<article class=\"product_pod\">\\n       ...'>,\n",
            " <Selector query=\"descendant-or-self::article[@class and contains(concat(' ', normalize-space(@class), ' '), ' product_pod ')]\" data='<article class=\"product_pod\">\\n       ...'>,\n",
            " <Selector query=\"descendant-or-self::article[@class and contains(concat(' ', normalize-space(@class), ' '), ' product_pod ')]\" data='<article class=\"product_pod\">\\n       ...'>,\n",
            " <Selector query=\"descendant-or-self::article[@class and contains(concat(' ', normalize-space(@class), ' '), ' product_pod ')]\" data='<article class=\"product_pod\">\\n       ...'>,\n",
            " <Selector query=\"descendant-or-self::article[@class and contains(concat(' ', normalize-space(@class), ' '), ' product_pod ')]\" data='<article class=\"product_pod\">\\n       ...'>,\n",
            " <Selector query=\"descendant-or-self::article[@class and contains(concat(' ', normalize-space(@class), ' '), ' product_pod ')]\" data='<article class=\"product_pod\">\\n       ...'>,\n",
            " <Selector query=\"descendant-or-self::article[@class and contains(concat(' ', normalize-space(@class), ' '), ' product_pod ')]\" data='<article class=\"product_pod\">\\n       ...'>,\n",
            " <Selector query=\"descendant-or-self::article[@class and contains(concat(' ', normalize-space(@class), ' '), ' product_pod ')]\" data='<article class=\"product_pod\">\\n       ...'>,\n",
            " <Selector query=\"descendant-or-self::article[@class and contains(concat(' ', normalize-space(@class), ' '), ' product_pod ')]\" data='<article class=\"product_pod\">\\n       ...'>,\n",
            " <Selector query=\"descendant-or-self::article[@class and contains(concat(' ', normalize-space(@class), ' '), ' product_pod ')]\" data='<article class=\"product_pod\">\\n       ...'>,\n",
            " <Selector query=\"descendant-or-self::article[@class and contains(concat(' ', normalize-space(@class), ' '), ' product_pod ')]\" data='<article class=\"product_pod\">\\n       ...'>,\n",
            " <Selector query=\"descendant-or-self::article[@class and contains(concat(' ', normalize-space(@class), ' '), ' product_pod ')]\" data='<article class=\"product_pod\">\\n       ...'>,\n",
            " <Selector query=\"descendant-or-self::article[@class and contains(concat(' ', normalize-space(@class), ' '), ' product_pod ')]\" data='<article class=\"product_pod\">\\n       ...'>,\n",
            " <Selector query=\"descendant-or-self::article[@class and contains(concat(' ', normalize-space(@class), ' '), ' product_pod ')]\" data='<article class=\"product_pod\">\\n       ...'>,\n",
            " <Selector query=\"descendant-or-self::article[@class and contains(concat(' ', normalize-space(@class), ' '), ' product_pod ')]\" data='<article class=\"product_pod\">\\n       ...'>,\n",
            " <Selector query=\"descendant-or-self::article[@class and contains(concat(' ', normalize-space(@class), ' '), ' product_pod ')]\" data='<article class=\"product_pod\">\\n       ...'>,\n",
            " <Selector query=\"descendant-or-self::article[@class and contains(concat(' ', normalize-space(@class), ' '), ' product_pod ')]\" data='<article class=\"product_pod\">\\n       ...'>,\n",
            " <Selector query=\"descendant-or-self::article[@class and contains(concat(' ', normalize-space(@class), ' '), ' product_pod ')]\" data='<article class=\"product_pod\">\\n       ...'>,\n",
            " <Selector query=\"descendant-or-self::article[@class and contains(concat(' ', normalize-space(@class), ' '), ' product_pod ')]\" data='<article class=\"product_pod\">\\n       ...'>,\n",
            " <Selector query=\"descendant-or-self::article[@class and contains(concat(' ', normalize-space(@class), ' '), ' product_pod ')]\" data='<article class=\"product_pod\">\\n       ...'>]\n",
            "\n",
            "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m4\u001b[0;38;5;28m]: \u001b[8D\u001b[8C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[8D\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m4\u001b[0;38;5;28m]: \u001b[0mresponse.css(\u001b[0;38;5;130m'article.product_pod'\u001b[0m).get ()\u001b[50D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[0m\u001b[?7h\u001b[0;38;5;88mOut[\u001b[0;91;1m4\u001b[0;38;5;88m]: \u001b[0m\u001b[0m'<article class=\"product_pod\">\\n        \\n            <div class=\"image_container\">\\n                \\n                    \\n                    <a href=\"catalogue/a-light-in-the-attic_1000/index.html\"><img src=\"media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg\" alt=\"A Light in the Attic\" class=\"thumbnail\"></a>\\n                    \\n                \\n            </div>\\n        \\n\\n        \\n            \\n                <p class=\"star-rating Three\">\\n                    <i class=\"icon-star\"></i>\\n                    <i class=\"icon-star\"></i>\\n                    <i class=\"icon-star\"></i>\\n                    <i class=\"icon-star\"></i>\\n                    <i class=\"icon-star\"></i>\\n                </p>\\n            \\n        \\n\\n        \\n            <h3><a href=\"catalogue/a-light-in-the-attic_1000/index.html\" title=\"A Light in the Attic\">A Light in the ...</a></h3>\\n        \\n\\n        \\n            <div class=\"product_price\">\\n                \\n\\n\\n\\n\\n\\n\\n    \\n        <p class=\"price_color\">£51.77</p>\\n    \\n\\n<p class=\"instock availability\">\\n    <i class=\"icon-ok\"></i>\\n    \\n        In stock\\n    \\n</p>\\n\\n                \\n                    \\n\\n\\n\\n\\n\\n\\n    \\n    <form>\\n        <button type=\"submit\" class=\"btn btn-primary btn-block\" data-loading-text=\"Adding...\">Add to basket</button>\\n    </form>\\n\\n\\n                \\n            </div>\\n        \\n    </article>'\n",
            "\n",
            "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m5\u001b[0;38;5;28m]: \u001b[8D\u001b[8C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[8D\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m5\u001b[0;38;5;28m]: \u001b[0mbooks = response.css(\u001b[0;38;5;130m'article.product_pod'\u001b[0m)\u001b[51D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\n",
            "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m6\u001b[0;38;5;28m]: \u001b[8D\u001b[8C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[8D\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m6\u001b[0;38;5;28m]: \u001b[0;38;5;28mlen\u001b[0m(books)\u001b[18D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[0m\u001b[?7h\u001b[0;38;5;88mOut[\u001b[0;91;1m6\u001b[0;38;5;88m]: \u001b[0m\u001b[0m20\n",
            "\n",
            "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m7\u001b[0;38;5;28m]: \u001b[8D\u001b[8C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[8D\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m7\u001b[0;38;5;28m]: \u001b[0mbook = books[\u001b[0;38;5;28m0\u001b[0m]\u001b[23D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\n",
            "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m8\u001b[0;38;5;28m]: \u001b[8D\u001b[8C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[8D\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m8\u001b[0;38;5;28m]: \u001b[0mbook.css(\u001b[0;38;5;130m'h3 a::text'\u001b[0m).get()\u001b[36D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[0m\u001b[?7h\u001b[0;38;5;88mOut[\u001b[0;91;1m8\u001b[0;38;5;88m]: \u001b[0m\u001b[0m'A Light in the ...'\n",
            "\n",
            "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m9\u001b[0;38;5;28m]: \u001b[8D\u001b[8C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[8D\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m9\u001b[0;38;5;28m]: \u001b[0mbook.css(\u001b[0;38;5;130m'.product_price .price_color::text'\u001b[0m).get()\u001b[59D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[0m\u001b[?7h\u001b[0;38;5;88mOut[\u001b[0;91;1m9\u001b[0;38;5;88m]: \u001b[0m\u001b[0m'£51.77'\n",
            "\n",
            "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m10\u001b[0;38;5;28m]: \u001b[9D\u001b[9C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[9D\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m10\u001b[0;38;5;28m]: \u001b[0mbook.css(\u001b[0;38;5;130m'h3 a'\u001b[0m).attrib[\u001b[0;38;5;130m'href'\u001b[0m]\u001b[40D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[0m\u001b[?7h\u001b[0;38;5;88mOut[\u001b[0;91;1m10\u001b[0;38;5;88m]: \u001b[0m\u001b[0m'catalogue/a-light-in-the-attic_1000/index.html'\n",
            "\n",
            "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m11\u001b[0;38;5;28m]: \u001b[9D\u001b[9C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[9D\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m11\u001b[0;38;5;28m]: \u001b[0mexit\u001b[13D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "fetch('https://books.toscrape.com/')\n",
        "\n",
        "response\n",
        "\n",
        "response.css('article.product_pod')\n",
        "\n",
        "response.css('article.product_pod').get ()\n",
        "\n",
        "books = response.css('article.product_pod')\n",
        "\n",
        "len(books)\n",
        "\n",
        "book = books[0]\n",
        "\n",
        "book.css('h3 a::text').get()\n",
        "\n",
        "book.css('.product_price .price_color::text').get()\n",
        "\n",
        "book.css('h3 a').attrib['href']\n",
        "\n",
        "exit"
      ],
      "metadata": {
        "id": "HInuInDQGnSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd ../"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03Y8hCpjKF4o",
        "outputId": "add9d2b8-c9df-42aa-93da-89cbbbe67cbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/part2/bookscraper/bookscraper\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile spiders/bookspider.py\n",
        "!import scrapy\n",
        "\n",
        "class BookspiderSpider(scrapy.Spider):\n",
        "    name = \"bookspider\"\n",
        "    allowed_domains = [\"books.toscrape.com\"]\n",
        "    start_urls = [\"https://books.toscrape.com\"]\n",
        "\n",
        "    def parse(self, response):\n",
        "        books = response.css('article.product_pod')\n",
        "        for book in books:\n",
        "            yield {\n",
        "                'title': book.css('h3 a::text').get(),\n",
        "                'price': book.css('.product_price .price_color::text').get(),\n",
        "                'url':book.css('h3 a').attrib['href'],\n",
        "            }"
      ],
      "metadata": {
        "id": "VE_6oC5tqMAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy crawl bookspider"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hHL3G_1Kcqe",
        "outputId": "b18e766c-53ea-422c-9f51-3cf51628c40f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-08 10:45:43 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: bookscraper)\n",
            "2025-02-08 10:45:43 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.3, Platform Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "2025-02-08 10:45:43 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2025-02-08 10:45:43 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2025-02-08 10:45:43 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2025-02-08 10:45:43 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2025-02-08 10:45:43 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2025-02-08 10:45:43 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2025-02-08 10:45:43 [scrapy.extensions.telnet] INFO: Telnet Password: c58ce3f59bd010c7\n",
            "2025-02-08 10:45:43 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2025-02-08 10:45:43 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'bookscraper',\n",
            " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'NEWSPIDER_MODULE': 'bookscraper.spiders',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['bookscraper.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2025-02-08 10:45:44 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2025-02-08 10:45:44 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2025-02-08 10:45:44 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2025-02-08 10:45:44 [scrapy.core.engine] INFO: Spider opened\n",
            "2025-02-08 10:45:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2025-02-08 10:45:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2025-02-08 10:45:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://books.toscrape.com/robots.txt> (referer: None)\n",
            "2025-02-08 10:45:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://books.toscrape.com> (referer: None)\n",
            "2025-02-08 10:45:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com>\n",
            "{'name': 'A Light in the ...', 'price': '£51.77', 'url': 'catalogue/a-light-in-the-attic_1000/index.html'}\n",
            "2025-02-08 10:45:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com>\n",
            "{'name': 'Tipping the Velvet', 'price': '£53.74', 'url': 'catalogue/tipping-the-velvet_999/index.html'}\n",
            "2025-02-08 10:45:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com>\n",
            "{'name': 'Soumission', 'price': '£50.10', 'url': 'catalogue/soumission_998/index.html'}\n",
            "2025-02-08 10:45:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com>\n",
            "{'name': 'Sharp Objects', 'price': '£47.82', 'url': 'catalogue/sharp-objects_997/index.html'}\n",
            "2025-02-08 10:45:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com>\n",
            "{'name': 'Sapiens: A Brief History ...', 'price': '£54.23', 'url': 'catalogue/sapiens-a-brief-history-of-humankind_996/index.html'}\n",
            "2025-02-08 10:45:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com>\n",
            "{'name': 'The Requiem Red', 'price': '£22.65', 'url': 'catalogue/the-requiem-red_995/index.html'}\n",
            "2025-02-08 10:45:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com>\n",
            "{'name': 'The Dirty Little Secrets ...', 'price': '£33.34', 'url': 'catalogue/the-dirty-little-secrets-of-getting-your-dream-job_994/index.html'}\n",
            "2025-02-08 10:45:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com>\n",
            "{'name': 'The Coming Woman: A ...', 'price': '£17.93', 'url': 'catalogue/the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html'}\n",
            "2025-02-08 10:45:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com>\n",
            "{'name': 'The Boys in the ...', 'price': '£22.60', 'url': 'catalogue/the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html'}\n",
            "2025-02-08 10:45:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com>\n",
            "{'name': 'The Black Maria', 'price': '£52.15', 'url': 'catalogue/the-black-maria_991/index.html'}\n",
            "2025-02-08 10:45:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com>\n",
            "{'name': 'Starving Hearts (Triangular Trade ...', 'price': '£13.99', 'url': 'catalogue/starving-hearts-triangular-trade-trilogy-1_990/index.html'}\n",
            "2025-02-08 10:45:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com>\n",
            "{'name': \"Shakespeare's Sonnets\", 'price': '£20.66', 'url': 'catalogue/shakespeares-sonnets_989/index.html'}\n",
            "2025-02-08 10:45:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com>\n",
            "{'name': 'Set Me Free', 'price': '£17.46', 'url': 'catalogue/set-me-free_988/index.html'}\n",
            "2025-02-08 10:45:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com>\n",
            "{'name': \"Scott Pilgrim's Precious Little ...\", 'price': '£52.29', 'url': 'catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html'}\n",
            "2025-02-08 10:45:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com>\n",
            "{'name': 'Rip it Up and ...', 'price': '£35.02', 'url': 'catalogue/rip-it-up-and-start-again_986/index.html'}\n",
            "2025-02-08 10:45:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com>\n",
            "{'name': 'Our Band Could Be ...', 'price': '£57.25', 'url': 'catalogue/our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html'}\n",
            "2025-02-08 10:45:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com>\n",
            "{'name': 'Olio', 'price': '£23.88', 'url': 'catalogue/olio_984/index.html'}\n",
            "2025-02-08 10:45:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com>\n",
            "{'name': 'Mesaerion: The Best Science ...', 'price': '£37.59', 'url': 'catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html'}\n",
            "2025-02-08 10:45:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com>\n",
            "{'name': 'Libertarianism for Beginners', 'price': '£51.33', 'url': 'catalogue/libertarianism-for-beginners_982/index.html'}\n",
            "2025-02-08 10:45:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com>\n",
            "{'name': \"It's Only the Himalayas\", 'price': '£45.17', 'url': 'catalogue/its-only-the-himalayas_981/index.html'}\n",
            "2025-02-08 10:45:44 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2025-02-08 10:45:44 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 460,\n",
            " 'downloader/request_count': 2,\n",
            " 'downloader/request_method_count/GET': 2,\n",
            " 'downloader/response_bytes': 51883,\n",
            " 'downloader/response_count': 2,\n",
            " 'downloader/response_status_count/200': 1,\n",
            " 'downloader/response_status_count/404': 1,\n",
            " 'elapsed_time_seconds': 0.438307,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2025, 2, 8, 10, 45, 44, 544940, tzinfo=datetime.timezone.utc),\n",
            " 'item_scraped_count': 20,\n",
            " 'items_per_minute': None,\n",
            " 'log_count/DEBUG': 27,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 294563840,\n",
            " 'memusage/startup': 294563840,\n",
            " 'response_received_count': 2,\n",
            " 'responses_per_minute': None,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/404': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2025, 2, 8, 10, 45, 44, 106633, tzinfo=datetime.timezone.utc)}\n",
            "2025-02-08 10:45:44 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy shell"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PVH_xeALQqs",
        "outputId": "31a92fd9-072c-40a6-85a8-185ebc5d9997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-08 10:49:05 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: bookscraper)\n",
            "2025-02-08 10:49:05 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.3, Platform Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "2025-02-08 10:49:05 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2025-02-08 10:49:05 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2025-02-08 10:49:05 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2025-02-08 10:49:05 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2025-02-08 10:49:05 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2025-02-08 10:49:05 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2025-02-08 10:49:05 [scrapy.extensions.telnet] INFO: Telnet Password: 35f975bf7ea0ee66\n",
            "2025-02-08 10:49:05 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage']\n",
            "2025-02-08 10:49:05 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'bookscraper',\n",
            " 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',\n",
            " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'LOGSTATS_INTERVAL': 0,\n",
            " 'NEWSPIDER_MODULE': 'bookscraper.spiders',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['bookscraper.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2025-02-08 10:49:05 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2025-02-08 10:49:05 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2025-02-08 10:49:05 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2025-02-08 10:49:05 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2025-02-08 10:49:05 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "\u001b[?12l\u001b[?25h[s] Available Scrapy objects:\n",
            "[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n",
            "[s]   crawler    <scrapy.crawler.Crawler object at 0x133fa8f5c8d0>\n",
            "[s]   item       {}\n",
            "[s]   settings   <scrapy.settings.Settings object at 0x133fa8501dd0>\n",
            "[s] Useful shortcuts:\n",
            "[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\n",
            "[s]   fetch(req)                  Fetch a scrapy.Request and update local objects \n",
            "[s]   shelp()           Shell help (print this help)\n",
            "[s]   view(response)    View response in a browser\n",
            "\u001b[6n\u001b[?2004h\u001b[?1l\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m1\u001b[0;38;5;28m]: \u001b[8D\u001b[8C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[8D\u001b[J\u001b[0m\u001b[?7h\u001b[?2004lWARNING: your terminal doesn't support cursor position requests (CPR).\n",
            "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m1\u001b[0;38;5;28m]: \u001b[8D\u001b[8C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[0mfetch(\u001b[0;38;5;130m'https://books.toscrape.com/'\u001b[0m)\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[44D\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m1\u001b[0;38;5;28m]: \u001b[0mfetch(\u001b[0;38;5;130m'https://books.toscrape.com/'\u001b[0m)\u001b[44D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l2025-02-08 10:49:54 [scrapy.core.engine] INFO: Spider opened\n",
            "2025-02-08 10:49:54 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://books.toscrape.com/robots.txt> (referer: None)\n",
            "2025-02-08 10:49:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://books.toscrape.com/> (referer: None)\n",
            "\n",
            "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m2\u001b[0;38;5;28m]: \u001b[8D\u001b[8C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[0mresponse.css(\u001b[0;38;5;130m'li.next a ::attr(href)'\u001b[0m).get()\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[52D\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m2\u001b[0;38;5;28m]: \u001b[0mresponse.css(\u001b[0;38;5;130m'li.next a ::attr(href)'\u001b[0m).get()\u001b[52D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[0m\u001b[?7h\u001b[0;38;5;88mOut[\u001b[0;91;1m2\u001b[0;38;5;88m]: \u001b[0m\u001b[0m'catalogue/page-2.html'\n",
            "\n",
            "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m3\u001b[0;38;5;28m]: \u001b[8D\u001b[8C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[8D\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m3\u001b[0;38;5;28m]: \u001b[0mexit\u001b[12D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "fetch('https://books.toscrape.com/')\n",
        "\n",
        "response.css('li.next a ::attr(href)').get()\n",
        "\n",
        "exit"
      ],
      "metadata": {
        "id": "OZweJJzPLZ0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile spiders/bookspider.py\n",
        "!import scrapy\n",
        "\n",
        "class BookspiderSpider(scrapy.Spider):\n",
        "    name = \"bookspider\"\n",
        "    allowed_domains = [\"books.toscrape.com\"]\n",
        "    start_urls = [\"https://books.toscrape.com\"]\n",
        "\n",
        "    def parse(self, response):\n",
        "        books = response.css('article.product_pod')\n",
        "        for book in books:\n",
        "            yield {\n",
        "                'title': book.css('h3 a::text').get(),\n",
        "                'price': book.css('.product_price .price_color::text').get(),\n",
        "                'url':book.css('h3 a').attrib['href'],\n",
        "            }\n",
        "        next_page = response.css('li.next a ::attr(href)').get()\n",
        "\n",
        "        if next_page is not None:\n",
        "          if 'catalogue/' in next_page:\n",
        "            next_page_url = 'https://books.toscrape.com/'+next_page\n",
        "          else:\n",
        "            next_page_url = 'https://books.toscrape.com/catalogue/'+next_page\n",
        "          yield response.follow(next_page_url, callback= self.parse)\n"
      ],
      "metadata": {
        "id": "VzBQjsOGqit_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy crawl bookspider"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SfiKPtzOBM_",
        "outputId": "a70b1926-47d8-4d4d-e2ae-26d518e45a15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-08 11:01:17 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: bookscraper)\n",
            "2025-02-08 11:01:17 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.3, Platform Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "2025-02-08 11:01:17 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2025-02-08 11:01:17 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2025-02-08 11:01:17 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2025-02-08 11:01:17 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2025-02-08 11:01:17 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2025-02-08 11:01:17 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2025-02-08 11:01:17 [scrapy.extensions.telnet] INFO: Telnet Password: 613b5ce2acb5ca68\n",
            "2025-02-08 11:01:17 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2025-02-08 11:01:17 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'bookscraper',\n",
            " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'NEWSPIDER_MODULE': 'bookscraper.spiders',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['bookscraper.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2025-02-08 11:01:17 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2025-02-08 11:01:17 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2025-02-08 11:01:17 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2025-02-08 11:01:17 [scrapy.core.engine] INFO: Spider opened\n",
            "2025-02-08 11:01:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2025-02-08 11:01:17 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2025-02-08 11:01:17 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://books.toscrape.com/robots.txt> (referer: None)\n",
            "2025-02-08 11:01:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://books.toscrape.com/> (referer: None)\n",
            "2025-02-08 11:01:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'A Light in the ...', 'price': '£51.77', 'url': 'catalogue/a-light-in-the-attic_1000/index.html'}\n",
            "2025-02-08 11:01:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'Tipping the Velvet', 'price': '£53.74', 'url': 'catalogue/tipping-the-velvet_999/index.html'}\n",
            "2025-02-08 11:01:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'Soumission', 'price': '£50.10', 'url': 'catalogue/soumission_998/index.html'}\n",
            "2025-02-08 11:01:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'Sharp Objects', 'price': '£47.82', 'url': 'catalogue/sharp-objects_997/index.html'}\n",
            "2025-02-08 11:01:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'Sapiens: A Brief History ...', 'price': '£54.23', 'url': 'catalogue/sapiens-a-brief-history-of-humankind_996/index.html'}\n",
            "2025-02-08 11:01:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'The Requiem Red', 'price': '£22.65', 'url': 'catalogue/the-requiem-red_995/index.html'}\n",
            "2025-02-08 11:01:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'The Dirty Little Secrets ...', 'price': '£33.34', 'url': 'catalogue/the-dirty-little-secrets-of-getting-your-dream-job_994/index.html'}\n",
            "2025-02-08 11:01:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'The Coming Woman: A ...', 'price': '£17.93', 'url': 'catalogue/the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html'}\n",
            "2025-02-08 11:01:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'The Boys in the ...', 'price': '£22.60', 'url': 'catalogue/the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html'}\n",
            "2025-02-08 11:01:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'The Black Maria', 'price': '£52.15', 'url': 'catalogue/the-black-maria_991/index.html'}\n",
            "2025-02-08 11:01:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'Starving Hearts (Triangular Trade ...', 'price': '£13.99', 'url': 'catalogue/starving-hearts-triangular-trade-trilogy-1_990/index.html'}\n",
            "2025-02-08 11:01:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': \"Shakespeare's Sonnets\", 'price': '£20.66', 'url': 'catalogue/shakespeares-sonnets_989/index.html'}\n",
            "2025-02-08 11:01:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'Set Me Free', 'price': '£17.46', 'url': 'catalogue/set-me-free_988/index.html'}\n",
            "2025-02-08 11:01:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': \"Scott Pilgrim's Precious Little ...\", 'price': '£52.29', 'url': 'catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html'}\n",
            "2025-02-08 11:01:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'Rip it Up and ...', 'price': '£35.02', 'url': 'catalogue/rip-it-up-and-start-again_986/index.html'}\n",
            "2025-02-08 11:01:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'Our Band Could Be ...', 'price': '£57.25', 'url': 'catalogue/our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html'}\n",
            "2025-02-08 11:01:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'Olio', 'price': '£23.88', 'url': 'catalogue/olio_984/index.html'}\n",
            "2025-02-08 11:01:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'Mesaerion: The Best Science ...', 'price': '£37.59', 'url': 'catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html'}\n",
            "2025-02-08 11:01:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'Libertarianism for Beginners', 'price': '£51.33', 'url': 'catalogue/libertarianism-for-beginners_982/index.html'}\n",
            "2025-02-08 11:01:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': \"It's Only the Himalayas\", 'price': '£45.17', 'url': 'catalogue/its-only-the-himalayas_981/index.html'}\n",
            "2025-02-08 11:01:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://books.toscrape.com/> (referer: None)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/utils/defer.py\", line 327, in iter_errback\n",
            "    yield next(it)\n",
            "          ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/utils/python.py\", line 368, in __next__\n",
            "    return next(self.data)\n",
            "           ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/utils/python.py\", line 368, in __next__\n",
            "    return next(self.data)\n",
            "           ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
            "    yield from iterable\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/spidermiddlewares/referer.py\", line 379, in <genexpr>\n",
            "    return (self._set_referer(r, response) for r in result)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
            "    yield from iterable\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/spidermiddlewares/urllength.py\", line 57, in <genexpr>\n",
            "    return (r for r in result if self._filter(r, spider))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
            "    yield from iterable\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/spidermiddlewares/depth.py\", line 54, in <genexpr>\n",
            "    return (r for r in result if self._filter(r, response, spider))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
            "    yield from iterable\n",
            "  File \"/content/part2/bookscraper/bookscraper/spiders/bookspider.py\", line 23, in parse\n",
            "    yield response.follow(next_page_url,callbak= self.parse)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: TextResponse.follow() got an unexpected keyword argument 'callbak'\n",
            "2025-02-08 11:01:18 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2025-02-08 11:01:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 460,\n",
            " 'downloader/request_count': 2,\n",
            " 'downloader/request_method_count/GET': 2,\n",
            " 'downloader/response_bytes': 51883,\n",
            " 'downloader/response_count': 2,\n",
            " 'downloader/response_status_count/200': 1,\n",
            " 'downloader/response_status_count/404': 1,\n",
            " 'elapsed_time_seconds': 0.473633,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2025, 2, 8, 11, 1, 18, 91612, tzinfo=datetime.timezone.utc),\n",
            " 'item_scraped_count': 20,\n",
            " 'items_per_minute': None,\n",
            " 'log_count/DEBUG': 27,\n",
            " 'log_count/ERROR': 1,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 294563840,\n",
            " 'memusage/startup': 294563840,\n",
            " 'response_received_count': 2,\n",
            " 'responses_per_minute': None,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/404': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'spider_exceptions/TypeError': 1,\n",
            " 'start_time': datetime.datetime(2025, 2, 8, 11, 1, 17, 617979, tzinfo=datetime.timezone.utc)}\n",
            "2025-02-08 11:01:18 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy crawl bookspider"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcnTX7ClQgFW",
        "outputId": "e4146ec8-5c1f-4a91-ef3f-b10997e232ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-08 11:11:40 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: bookscraper)\n",
            "2025-02-08 11:11:40 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.3, Platform Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "2025-02-08 11:11:40 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2025-02-08 11:11:40 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2025-02-08 11:11:40 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2025-02-08 11:11:40 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2025-02-08 11:11:40 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2025-02-08 11:11:40 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2025-02-08 11:11:40 [scrapy.extensions.telnet] INFO: Telnet Password: 0a28f89ffffede71\n",
            "2025-02-08 11:11:40 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2025-02-08 11:11:40 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'bookscraper',\n",
            " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'NEWSPIDER_MODULE': 'bookscraper.spiders',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['bookscraper.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2025-02-08 11:11:40 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2025-02-08 11:11:40 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2025-02-08 11:11:40 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2025-02-08 11:11:40 [scrapy.core.engine] INFO: Spider opened\n",
            "2025-02-08 11:11:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2025-02-08 11:11:40 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2025-02-08 11:11:40 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://books.toscrape.com/robots.txt> (referer: None)\n",
            "2025-02-08 11:11:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://books.toscrape.com/> (referer: None)\n",
            "2025-02-08 11:11:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'A Light in the ...', 'price': '£51.77', 'url': 'catalogue/a-light-in-the-attic_1000/index.html'}\n",
            "2025-02-08 11:11:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'Tipping the Velvet', 'price': '£53.74', 'url': 'catalogue/tipping-the-velvet_999/index.html'}\n",
            "2025-02-08 11:11:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'Soumission', 'price': '£50.10', 'url': 'catalogue/soumission_998/index.html'}\n",
            "2025-02-08 11:11:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'Sharp Objects', 'price': '£47.82', 'url': 'catalogue/sharp-objects_997/index.html'}\n",
            "2025-02-08 11:11:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'Sapiens: A Brief History ...', 'price': '£54.23', 'url': 'catalogue/sapiens-a-brief-history-of-humankind_996/index.html'}\n",
            "2025-02-08 11:11:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'The Requiem Red', 'price': '£22.65', 'url': 'catalogue/the-requiem-red_995/index.html'}\n",
            "2025-02-08 11:11:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'The Dirty Little Secrets ...', 'price': '£33.34', 'url': 'catalogue/the-dirty-little-secrets-of-getting-your-dream-job_994/index.html'}\n",
            "2025-02-08 11:11:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'The Coming Woman: A ...', 'price': '£17.93', 'url': 'catalogue/the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html'}\n",
            "2025-02-08 11:11:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'The Boys in the ...', 'price': '£22.60', 'url': 'catalogue/the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html'}\n",
            "2025-02-08 11:11:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'The Black Maria', 'price': '£52.15', 'url': 'catalogue/the-black-maria_991/index.html'}\n",
            "2025-02-08 11:11:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'Starving Hearts (Triangular Trade ...', 'price': '£13.99', 'url': 'catalogue/starving-hearts-triangular-trade-trilogy-1_990/index.html'}\n",
            "2025-02-08 11:11:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': \"Shakespeare's Sonnets\", 'price': '£20.66', 'url': 'catalogue/shakespeares-sonnets_989/index.html'}\n",
            "2025-02-08 11:11:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'Set Me Free', 'price': '£17.46', 'url': 'catalogue/set-me-free_988/index.html'}\n",
            "2025-02-08 11:11:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': \"Scott Pilgrim's Precious Little ...\", 'price': '£52.29', 'url': 'catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html'}\n",
            "2025-02-08 11:11:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'Rip it Up and ...', 'price': '£35.02', 'url': 'catalogue/rip-it-up-and-start-again_986/index.html'}\n",
            "2025-02-08 11:11:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'Our Band Could Be ...', 'price': '£57.25', 'url': 'catalogue/our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html'}\n",
            "2025-02-08 11:11:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'Olio', 'price': '£23.88', 'url': 'catalogue/olio_984/index.html'}\n",
            "2025-02-08 11:11:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'Mesaerion: The Best Science ...', 'price': '£37.59', 'url': 'catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html'}\n",
            "2025-02-08 11:11:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': 'Libertarianism for Beginners', 'price': '£51.33', 'url': 'catalogue/libertarianism-for-beginners_982/index.html'}\n",
            "2025-02-08 11:11:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://books.toscrape.com/>\n",
            "{'name': \"It's Only the Himalayas\", 'price': '£45.17', 'url': 'catalogue/its-only-the-himalayas_981/index.html'}\n",
            "2025-02-08 11:11:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://books.toscrape.com/> (referer: None)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/utils/defer.py\", line 327, in iter_errback\n",
            "    yield next(it)\n",
            "          ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/utils/python.py\", line 368, in __next__\n",
            "    return next(self.data)\n",
            "           ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/utils/python.py\", line 368, in __next__\n",
            "    return next(self.data)\n",
            "           ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
            "    yield from iterable\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/spidermiddlewares/referer.py\", line 379, in <genexpr>\n",
            "    return (self._set_referer(r, response) for r in result)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
            "    yield from iterable\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/spidermiddlewares/urllength.py\", line 57, in <genexpr>\n",
            "    return (r for r in result if self._filter(r, spider))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
            "    yield from iterable\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/spidermiddlewares/depth.py\", line 54, in <genexpr>\n",
            "    return (r for r in result if self._filter(r, response, spider))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
            "    yield from iterable\n",
            "  File \"/content/part2/bookscraper/bookscraper/spiders/bookspider.py\", line 26, in parse\n",
            "    yield response.follow(next_page_url,callbak= self.parse)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: TextResponse.follow() got an unexpected keyword argument 'callbak'\n",
            "2025-02-08 11:11:40 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2025-02-08 11:11:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 460,\n",
            " 'downloader/request_count': 2,\n",
            " 'downloader/request_method_count/GET': 2,\n",
            " 'downloader/response_bytes': 51883,\n",
            " 'downloader/response_count': 2,\n",
            " 'downloader/response_status_count/200': 1,\n",
            " 'downloader/response_status_count/404': 1,\n",
            " 'elapsed_time_seconds': 0.402017,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2025, 2, 8, 11, 11, 40, 840542, tzinfo=datetime.timezone.utc),\n",
            " 'item_scraped_count': 20,\n",
            " 'items_per_minute': None,\n",
            " 'log_count/DEBUG': 27,\n",
            " 'log_count/ERROR': 1,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 294563840,\n",
            " 'memusage/startup': 294563840,\n",
            " 'response_received_count': 2,\n",
            " 'responses_per_minute': None,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/404': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'spider_exceptions/TypeError': 1,\n",
            " 'start_time': datetime.datetime(2025, 2, 8, 11, 11, 40, 438525, tzinfo=datetime.timezone.utc)}\n",
            "2025-02-08 11:11:40 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy shell"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ni0EDISEVvGx",
        "outputId": "c55730c8-ad0b-4a8f-f553-528d09dadc2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-08 12:11:15 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: bookscraper)\n",
            "2025-02-08 12:11:15 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.3, Platform Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "2025-02-08 12:11:15 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2025-02-08 12:11:15 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2025-02-08 12:11:15 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2025-02-08 12:11:15 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2025-02-08 12:11:15 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2025-02-08 12:11:15 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2025-02-08 12:11:15 [scrapy.extensions.telnet] INFO: Telnet Password: 9be2e4aa3ea63e3d\n",
            "2025-02-08 12:11:15 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage']\n",
            "2025-02-08 12:11:15 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'bookscraper',\n",
            " 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',\n",
            " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'LOGSTATS_INTERVAL': 0,\n",
            " 'NEWSPIDER_MODULE': 'bookscraper.spiders',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['bookscraper.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2025-02-08 12:11:15 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2025-02-08 12:11:15 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2025-02-08 12:11:15 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2025-02-08 12:11:15 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2025-02-08 12:11:15 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "\u001b[?12l\u001b[?25h[s] Available Scrapy objects:\n",
            "[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n",
            "[s]   crawler    <scrapy.crawler.Crawler object at 0xea984ae4610>\n",
            "[s]   item       {}\n",
            "[s]   settings   <scrapy.settings.Settings object at 0xea984088310>\n",
            "[s] Useful shortcuts:\n",
            "[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\n",
            "[s]   fetch(req)                  Fetch a scrapy.Request and update local objects \n",
            "[s]   shelp()           Shell help (print this help)\n",
            "[s]   view(response)    View response in a browser\n",
            "\u001b[6n\u001b[?2004h\u001b[?1l\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m1\u001b[0;38;5;28m]: \u001b[8D\u001b[8C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[8D\u001b[J\u001b[0m\u001b[?7h\u001b[?2004lWARNING: your terminal doesn't support cursor position requests (CPR).\n",
            "\u001b[79Cn\u001b[0m\n",
            "\u001b[79Cn\u001b[0m\n",
            "\u001b[0;38;5;28m   ...: \u001b[0;38;5;130mdex.html'\u001b[0m)\u001b[18D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l2025-02-08 12:11:45 [scrapy.core.engine] INFO: Spider opened\n",
            "2025-02-08 12:11:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://books.toscrape.com/robots.txt> (referer: None)\n",
            "2025-02-08 12:11:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html> (referer: None)\n",
            "\n",
            "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m2\u001b[0;38;5;28m]: \u001b[8D\u001b[8C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[0mresponse.css(\u001b[0;38;5;130m'.product_page'\u001b[0m)\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[37D\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m2\u001b[0;38;5;28m]: \u001b[0mresponse.css(\u001b[0;38;5;130m'.product_page'\u001b[0m)\u001b[37D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[0m\u001b[?7h\u001b[0;38;5;88mOut[\u001b[0;91;1m2\u001b[0;38;5;88m]: \u001b[0m\u001b[0m[<Selector query=\"descendant-or-self::*[@class and contains(concat(' ', normalize-space(@class), ' '), ' product_page ')]\" data='<article class=\"product_page\"><!-- St...'>]\n",
            "\n",
            "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m3\u001b[0;38;5;28m]: \u001b[8D\u001b[8C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[8D\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m3\u001b[0;38;5;28m]: \u001b[0mresponse.css(\u001b[0;38;5;130m'.product_main h1::text'\u001b[0m).get()\u001b[52D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[0m\u001b[?7h\u001b[0;38;5;88mOut[\u001b[0;91;1m3\u001b[0;38;5;88m]: \u001b[0m\u001b[0m'A Light in the Attic'\n",
            "\n",
            "\u001b[79Ce\u001b[0m\n",
            "\u001b[79Ce\u001b[0m\n",
            "\u001b[0;38;5;28m   ...: \u001b[0;38;5;130mxt()\"\u001b[0m).get()\u001b[20D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[0m\u001b[?7h\u001b[0;38;5;88mOut[\u001b[0;91;1m4\u001b[0;38;5;88m]: \u001b[0m\u001b[0m\"It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love th It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love that Silverstein. Need proof of his genius? RockabyeRockabye baby, in the treetopDon't you know a treetopIs no safe place to rock?And who put you up there,And your cradle, too?Baby, I think someone down here'sGot it in for you. Shel, you never sounded so good. ...more\"\n",
            "\n",
            "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m5\u001b[0;38;5;28m]: \u001b[8D\u001b[8C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[8D\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m5\u001b[0;38;5;28m]: \u001b[8D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\n",
            "\u001b[79C-\u001b[0m\n",
            "\u001b[79C-\u001b[0m\n",
            "\u001b[0;38;5;28m   ...: \u001b[0;38;5;130msibling::li[1]/a/text()\"\u001b[0m).get()\u001b[39D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[0m\u001b[?7h\u001b[0;38;5;88mOut[\u001b[0;91;1m5\u001b[0;38;5;88m]: \u001b[0m\u001b[0m'Poetry'\n",
            "\n",
            "\u001b[79C)\u001b[0m\n",
            "\u001b[79C)\u001b[0m\n",
            "\u001b[0;38;5;28m   ...: \u001b[0;38;5;130m\"\u001b[0m).get()\u001b[16D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
            "\u001b[0;32m<ipython-input-6-5cd9744b35b0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"//div[@id='product_description']/following-sibling::p/text()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'book' is not defined\n",
            "\n",
            "\u001b[79C)\u001b[0m\n",
            "\u001b[79C)\u001b[0m\n",
            "\u001b[0;38;5;28m   ...: \u001b[0;38;5;130m\"\u001b[0m).get()\u001b[16D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
            "\u001b[0;32m<ipython-input-7-5cd9744b35b0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"//div[@id='product_description']/following-sibling::p/text()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'book' is not defined\n",
            "\n",
            "\u001b[79Ce\u001b[0m\n",
            "\u001b[79Ce\u001b[0m\n",
            "\u001b[0;38;5;28m   ...: \u001b[0;38;5;130mxt()\"\u001b[0m).get()\u001b[20D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[0m\u001b[?7h\u001b[0;38;5;88mOut[\u001b[0;91;1m8\u001b[0;38;5;88m]: \u001b[0m\u001b[0m\"It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love th It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love that Silverstein. Need proof of his genius? RockabyeRockabye baby, in the treetopDon't you know a treetopIs no safe place to rock?And who put you up there,And your cradle, too?Baby, I think someone down here'sGot it in for you. Shel, you never sounded so good. ...more\"\n",
            "\n",
            "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m9\u001b[0;38;5;28m]: \u001b[8D\u001b[8C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[0mtable_rows = response.css(\u001b[0;38;5;130m\"table tr\"\u001b[0m)\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[45D\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m9\u001b[0;38;5;28m]: \u001b[0mtable_rows = response.css(\u001b[0;38;5;130m\"table tr\"\u001b[0m)\u001b[45D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\n",
            "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m10\u001b[0;38;5;28m]: \u001b[9D\u001b[9C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[9D\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m10\u001b[0;38;5;28m]: \u001b[0;38;5;28mlen\u001b[0m(table_rows)\u001b[24D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[0m\u001b[?7h\u001b[0;38;5;88mOut[\u001b[0;91;1m10\u001b[0;38;5;88m]: \u001b[0m\u001b[0m7\n",
            "\n",
            "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m11\u001b[0;38;5;28m]: \u001b[9D\u001b[9C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[9D\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m11\u001b[0;38;5;28m]: \u001b[0mtable_rows[\u001b[0;38;5;28m1\u001b[0m].css(\u001b[0;38;5;130m\"td ::text\"\u001b[0m).get()\u001b[45D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[0m\u001b[?7h\u001b[0;38;5;88mOut[\u001b[0;91;1m11\u001b[0;38;5;88m]: \u001b[0m\u001b[0m'Books'\n",
            "\n",
            "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m12\u001b[0;38;5;28m]: \u001b[9D\u001b[9C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[9D\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m12\u001b[0;38;5;28m]: \u001b[0mtable_rows[\u001b[0;38;5;28m2\u001b[0m].css(\u001b[0;38;5;130m\"td ::text\"\u001b[0m).get()\u001b[45D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[0m\u001b[?7h\u001b[0;38;5;88mOut[\u001b[0;91;1m12\u001b[0;38;5;88m]: \u001b[0m\u001b[0m'£51.77'\n",
            "\n",
            "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m13\u001b[0;38;5;28m]: \u001b[9D\u001b[9C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[9D\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m13\u001b[0;38;5;28m]: \u001b[0mresponse.css(\u001b[0;38;5;130m\"p.star-rating\"\u001b[0m).attrib[\u001b[0;38;5;130m'class'\u001b[0m]\u001b[54D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[0m\u001b[?7h\u001b[0;38;5;88mOut[\u001b[0;91;1m13\u001b[0;38;5;88m]: \u001b[0m\u001b[0m'star-rating Three'\n",
            "\n",
            "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m14\u001b[0;38;5;28m]: \u001b[9D\u001b[9C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[9D\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;92;1m14\u001b[0;38;5;28m]: \u001b[0mexit()\u001b[15D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "fetch('https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html')\n",
        "\n",
        "response.css('.product_page')\n",
        "\n",
        "response.css('.product_main h1::text').get()\n",
        "\n",
        "response.xpath(\"//div[@id='product_description']/following-sibling::p/text()\").get()\n",
        "\n",
        "response.xpath(\"//ul[@class='breadcrumb']/li[@class='active']/preceding-sibling::li[1]/a/text()\").get()\n",
        "\n",
        "response.xpath(\"//div[@id='product_description']/following-sibling::p/text()\").get()\n",
        "\n",
        "table_rows = response.css(\"table tr\")\n",
        "\n",
        "len(table_rows)\n",
        "table_rows[1].css(\"td ::text\").get()\n",
        "\n",
        "table_rows[2].css(\"td ::text\").get()\n",
        "response.css(\"p.star-rating\").attrib['class']\n",
        "\n",
        "exit()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XpTi3maoWTPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile spiders/bookspider.py\n",
        "!import scrapy\n",
        "\n",
        "class BookspiderSpider(scrapy.Spider):\n",
        "    name = \"bookspider\"\n",
        "    allowed_domains = [\"books.toscrape.com\"]\n",
        "    start_urls = [\"https://books.toscrape.com\"]\n",
        "\n",
        "    def parse(self, response):\n",
        "        books = response.css('article.product_pod')\n",
        "        for book in books:\n",
        "          relative_url = book.css('h3 a ::attr(href)').get()\n",
        "          if 'catalogue/' in relative_url:\n",
        "            book_url = 'https://books.toscrape.com/'+relative_url\n",
        "          else:\n",
        "            book_url = 'https://books.toscrape.com/catalogue/'+relative_url\n",
        "          yield response.follow(book_url, callback= self.parse_book_page)\n",
        "\n",
        "        next_page =response.css('li.next a ::attr(href)').get()\n",
        "        if next_page is not None:\n",
        "          if 'catalogue/' in next_page:\n",
        "            next_page_url = 'https://books.toscrape.com/'+next_page\n",
        "          else:\n",
        "            next_page_url = 'https://books.toscrape.com/catalogue/'+next_page\n",
        "          yield response.follow(next_page_url, callback= self.parse)\n",
        "\n",
        "    def parse_book_page(self, response):\n",
        "\n",
        "      table_rows = response.css(\"table tr\")\n",
        "      yield{\n",
        "        'url':response.url,\n",
        "        'title':response.css('.product_main h1::text').get(),\n",
        "        'product_type':table_rows[1].css('td ::text').get(),\n",
        "        'price_excl_tax':table_rows[2].css('td ::text').get(),\n",
        "        'price_incl_tax':table_rows[3].css('td ::text').get(),\n",
        "        'tax':table_rows[4].css('td ::text').get(),\n",
        "        'Availability':table_rows[5].css('td ::text').get(),\n",
        "        'Number of reviews':table_rows[6].css('td ::text').get(),\n",
        "        'stars':response.css(\"p.star-rating\").attrib['class'],\n",
        "        'category':response.xpath(\"//ul[@class='breadcrumb']/li[@class='active']/preceding-sibling::li[1]/a/text()\").get(),\n",
        "        'description':response.xpath(\"//div[@id='product_description']/following-sibling::p/text()\").get(),\n",
        "        'price':response.css('p.price_color::text').get(),}"
      ],
      "metadata": {
        "id": "bJMxL6qAq8G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy crawl bookspider -O bookdata.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkKCRmOcpHQX",
        "outputId": "6333f453-ec20-4f63-c16f-13609f65ffb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-08 12:59:12 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: bookscraper)\n",
            "2025-02-08 12:59:12 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.3, Platform Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "2025-02-08 12:59:12 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2025-02-08 12:59:12 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2025-02-08 12:59:12 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2025-02-08 12:59:12 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2025-02-08 12:59:12 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2025-02-08 12:59:12 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2025-02-08 12:59:12 [scrapy.extensions.telnet] INFO: Telnet Password: b75eaca7d252359d\n",
            "2025-02-08 12:59:12 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.feedexport.FeedExporter',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2025-02-08 12:59:12 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'bookscraper',\n",
            " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'NEWSPIDER_MODULE': 'bookscraper.spiders',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['bookscraper.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2025-02-08 12:59:12 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2025-02-08 12:59:12 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2025-02-08 12:59:12 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2025-02-08 12:59:12 [scrapy.core.engine] INFO: Spider opened\n",
            "2025-02-08 12:59:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2025-02-08 12:59:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2025-02-08 12:59:12 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://books.toscrape.com/robots.txt> (referer: None)\n",
            "2025-02-08 12:59:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://books.toscrape.com/> (referer: None)\n",
            "2025-02-08 12:59:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://books.toscrape.com/> (referer: None)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/utils/defer.py\", line 327, in iter_errback\n",
            "    yield next(it)\n",
            "          ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/utils/python.py\", line 368, in __next__\n",
            "    return next(self.data)\n",
            "           ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/utils/python.py\", line 368, in __next__\n",
            "    return next(self.data)\n",
            "           ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
            "    yield from iterable\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/spidermiddlewares/referer.py\", line 379, in <genexpr>\n",
            "    return (self._set_referer(r, response) for r in result)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
            "    yield from iterable\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/spidermiddlewares/urllength.py\", line 57, in <genexpr>\n",
            "    return (r for r in result if self._filter(r, spider))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
            "    yield from iterable\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/spidermiddlewares/depth.py\", line 54, in <genexpr>\n",
            "    return (r for r in result if self._filter(r, response, spider))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
            "    yield from iterable\n",
            "  File \"/content/part2/bookscraper/bookscraper/spiders/bookspider.py\", line 10, in parse\n",
            "    books = book.css('article.product_pod')\n",
            "            ^^^^\n",
            "UnboundLocalError: cannot access local variable 'book' where it is not associated with a value\n",
            "2025-02-08 12:59:13 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2025-02-08 12:59:13 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: bookdata.json\n",
            "2025-02-08 12:59:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 460,\n",
            " 'downloader/request_count': 2,\n",
            " 'downloader/request_method_count/GET': 2,\n",
            " 'downloader/response_bytes': 51883,\n",
            " 'downloader/response_count': 2,\n",
            " 'downloader/response_status_count/200': 1,\n",
            " 'downloader/response_status_count/404': 1,\n",
            " 'elapsed_time_seconds': 0.51185,\n",
            " 'feedexport/success_count/FileFeedStorage': 1,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2025, 2, 8, 12, 59, 13, 75135, tzinfo=datetime.timezone.utc),\n",
            " 'items_per_minute': None,\n",
            " 'log_count/DEBUG': 7,\n",
            " 'log_count/ERROR': 1,\n",
            " 'log_count/INFO': 11,\n",
            " 'memusage/max': 295366656,\n",
            " 'memusage/startup': 295366656,\n",
            " 'response_received_count': 2,\n",
            " 'responses_per_minute': None,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/404': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'spider_exceptions/UnboundLocalError': 1,\n",
            " 'start_time': datetime.datetime(2025, 2, 8, 12, 59, 12, 563285, tzinfo=datetime.timezone.utc)}\n",
            "2025-02-08 12:59:13 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    }
  ]
}